# 5-Day Gen AI Intensive Course

This repo stores the Kaggle 5-day Generative AI course notebooks

course video recording:
[Youtube livestream recording](https://www.youtube.com/watch?v=kpRyiJUUFxY&list=PLqFaTIg4myu-b1PlxitQdY0UYIbys-2es)


## Day 1 Foundational Large Language Models & Text Generation
Complete the Intro Unit - “Foundational Large Language Models & Text Generation”, which is:

➡️ [Optional] Listen to the [summary podcast episode](https://www.youtube.com/watch?v=mQDlCZZsOyo&t=0s) for this unit (created by NotebookLM, https://notebooklm.google.com/).

➡️ Read the “Foundational Large Language Models & Text Generation” [whitepaper](https://www.kaggle.com/whitepaper-foundational-llm-and-text-generation)

Complete Unit 1 - “Prompt Engineering”, which is:

➡️ [Optional] Listen to the summary podcast [episode](https://www.youtube.com/watch?v=F_hJ2Ey4BNc) for this unit (created by NotebookLM).

➡️ Read the “Prompt Engineering” [whitepaper](https://www.kaggle.com/whitepaper-prompt-engineering)

➡️ Complete this code lab on Kaggle where you’ll learn prompting fundamentals. Make sure you phone verify (https://www.kaggle.com/settings) your account before starting, it's necessary for the code labs.

[Day 1 - Prompting](https://www.kaggle.com/code/markishere/day-1-prompting)


## Day 2 Embeddings and Vector Stores/Databases
Resources mentioned in today's livestream:
Jinhyuk Lee's Google Scholar [profile](https://scholar.google.com/citations?user=YWm_zVcAAAAJ&hl=en)

The original transformer paper: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

BERT paper explaining bidirectional attention: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

A recent paper from NVidia explaining how to adapt decoder-only language model for embedding generation: [NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models](https://arxiv.org/abs/2405.17428)

Whitepaper explaining ["Native Integration of the ScaNN Algorithm into AlloyDB Database Internals"](https://services.google.com/fh/files/misc/scann_for_alloydb_whitepaper.pdf)

Assignments:

➡️ [Optional] Listen to the [summary podcast episode](https://www.youtube.com/watch?v=1CC39K76Nqs&t=0s) for this unit (created by NotebookLM, https://notebooklm.google.com/).

➡️ Read the “Embeddings and Vector Stores/Databases” [whitepaper](https://www.kaggle.com/whitepaper-embeddings-and-vector-stores)

➡️ Complete these code labs on Kaggle:
Build a RAG question-answering system over custom documents 

- [Day 2 - Document Q&A with RAG](https://www.kaggle.com/code/markishere/day-2-document-q-a-with-rag)

Explore text similarity with embeddings 

- [Day 2 - Embeddings and similarity scores](https://www.kaggle.com/code/markishere/day-2-embeddings-and-similarity-scores)

Build a neural classification network with Keras using embeddings 

- [Day 2 - Classifying embeddings with Keras](https://www.kaggle.com/code/markishere/day-2-classifying-embeddings-with-keras)

## day 3 Generative AI Agents

Agent building frameworks:  
https://www.langchain.com/langgraph
https://firebase.google.com/docs/genkit
https://github.com/breadboard-ai/brea...
 
What is RAG? high level page from GCP / Vertex:  
https://cloud.google.com/use-cases/re...


Complete Unit 3: “Generative AI Agents”, which is:
➡️ [Optional] Listen to the summary podcast [episode](https://www.youtube.com/watch?v=H4gZd4BCrDQ) for this unit (created by NotebookLM).

➡️ Read the [“Generative AI Agents”](https://www.kaggle.com/whitepaper-agents) whitepaper

➡️ Complete these code labs on Kaggle:
- [Talk to a database with function calling](https://www.kaggle.com/code/markishere/day-3-function-calling-with-the-gemini-api)

- [Day 3 - Building an agent with LangGraph](https://www.kaggle.com/code/markishere/day-3-building-an-agent-with-langgraph/)

## day 4 Domain-Specific LLMs
Complete Unit 4: “Domain-Specific LLMs”, which is:
➡️  [Optional] Listen to the summary podcast [episode](https://www.youtube.com/watch?v=b1a4ZOQ8XdI) for this unit (created by NotebookLM).

➡️  Read the [Solving Domain-Specific Problems Using LLMs](https://www.kaggle.com/whitepaper-solving-domains-specific-problems-using-llms) whitepaper

➡️  Complete these code labs on Kaggle:
[Optional] Use Google Search data in generation. (Note: Grounding with Google Search has been released as a limited launch and is not available in all locations. The EEA, UK, and CH regions will be supported at a later date)
- [Day 4 - Google Search grounding](https://www.kaggle.com/code/markishere/day-4-google-search-grounding)
- [Day 4 - Fine tuning a custom model](https://www.kaggle.com/code/markishere/day-4-fine-tuning-a-custom-model)


## day 5 MLOps for Generative AI
➡️ [Optional] Listen to the summary podcast [episode](https://www.youtube.com/watch?v=k9S6IhiUUj4) for this unit (created by NotebookLM, https://notebooklm.google/).

➡️ Read the “MLOps for Generative AI” [whitepaper](https://www.kaggle.com/whitepaper-operationalizing-generative-ai-on-vertex-ai-using-mlops)

➡️ No code lab for today! We will do a code walkthrough and live demo of goo.gle/e2e-gen-ai-app-starter-pack [link](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/sample-apps/e2e-gen-ai-app-starter-pack), a resource created for making MLOps for Gen AI easier and accelerating the path to production. Please go through the repository in advance.
